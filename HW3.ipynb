{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab354f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing, metrics\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0162900",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b27917",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10cd6ce",
   "metadata": {},
   "source": [
    "I use the PyTorch framework to implement my 2-layer Neural Network. I used the pytorch documentation available at https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html as reference when building my model. PyTorch makes it easier to build a model by subclassing the nn.Module class. You use the custom class to implement forward propagation and to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affbceb6",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db5ece0",
   "metadata": {},
   "source": [
    "## 1. Exploratory Data Analysis\n",
    "### Load data files and combine into 1 DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fa40940",
   "metadata": {},
   "outputs": [],
   "source": [
    "fds = [x[0] for x in os.walk('archive')][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "952631bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for fd in fds:\n",
    "    dfs.append(pd.read_csv(f\"{fd}\\\\vegas.txt\"))\n",
    "odds_df = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bb2871",
   "metadata": {},
   "source": [
    "### Drop unnessecary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d694f9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "odds_df.drop(columns=['PercentBet_OU', 'Open_Line_OU', 'Open_Odds_OU', 'Pinnacle_Line_OU',\n",
    "       'Pinnacle_Odds_OU', '5dimes_Line_OU', '5dimes_Odds_OU',\n",
    "       'Heritage_Line_OU', 'Heritage_Odds_OU', 'Bovada_Line_OU',\n",
    "       'Bovada_Odds_OU', 'Betonline_Line_OU', 'Betonline_Odds_OU', 'Average_Odds_OU', 'Best_Line_OU', 'Worst_Line_OU',\n",
    "       'Best_Odds_OU', 'Worst_Odds_OU',\n",
    "                     'Open_Line_Spread', 'Open_Odds_Spread', 'Pinnacle_Line_Spread',\n",
    "       'Pinnacle_Odds_Spread', '5dimes_Line_Spread', '5dimes_Odds_Spread',\n",
    "       'Heritage_Line_Spread', 'Heritage_Odds_Spread', 'Bovada_Line_Spread',\n",
    "       'Bovada_Odds_Spread', 'Betonline_Line_Spread', 'Betonline_Odds_Spread'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1dbd16",
   "metadata": {},
   "source": [
    "### Drop insufficient rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d88afedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "odds_df = odds_df[(~odds_df['Average_Line_ML'].isna()) * (odds_df['Average_Line_ML']!=0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac9a54f",
   "metadata": {},
   "source": [
    "### Replace n/a columns with average of other moneylines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdd461a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml = ['Open_Line_ML', 'Pinnacle_ML', '5dimes_ML',\n",
    "       'Heritage_ML', 'Bovada_ML', 'Betonline_ML']\n",
    "odds_df.loc[:, ml] = odds_df[ml].where(~odds_df[ml].isna(), odds_df['Average_Line_ML'], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa82d8f",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57ce58a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert MoneyLines to Fractional Odds.\n",
    "#This makes the range for these features (0,inf) as opposed to (-inf,-100] U [100,inf)\n",
    "\n",
    "lines = ['Open_Line_ML', 'Pinnacle_ML', '5dimes_ML',\n",
    "       'Heritage_ML', 'Bovada_ML', 'Betonline_ML', 'Average_Odds_Spread',\n",
    "        'Best_Odds_Spread', 'Worst_Odds_Spread', 'Average_Line_ML','Best_Line_ML','Worst_Line_ML' ]\n",
    "cond = odds_df[lines] > 0\n",
    "pos = (odds_df[lines]/100)\n",
    "neg = (-100/odds_df[lines])\n",
    "\n",
    "odds_df.loc[:,lines] = pos.where(cond, other=neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6538c4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond2 = odds_df['Result'] == 'W'\n",
    "odds_df.loc[cond2,'Result'] = 1\n",
    "odds_df.loc[~cond2,'Result'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c39b50b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "odds_df['Location'].replace(to_replace='home',value=1.0, inplace=True)\n",
    "odds_df['Location'].replace(to_replace='away',value=-1.0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f97eb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_features = ['PercentBet_ML', 'Open_Line_ML', 'Pinnacle_ML', '5dimes_ML',\n",
    "       'Heritage_ML', 'Bovada_ML', 'Betonline_ML', 'PercentBet_Spread',\n",
    "       'Average_Line_Spread', 'Average_Odds_Spread', 'Best_Line_Spread',\n",
    "       'Worst_Line_Spread', 'Best_Odds_Spread', 'Worst_Odds_Spread', \n",
    "       'Average_Line_OU']\n",
    "atributes = ['Pts', 'Spread', 'Result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53d49752",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler()\n",
    "odds_df.loc[:,scaled_features] = scaler.fit_transform(odds_df[scaled_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc84a6c",
   "metadata": {},
   "source": [
    "### See Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60a23e70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Location</th>\n",
       "      <th>Team</th>\n",
       "      <th>OppTeam</th>\n",
       "      <th>TeamId</th>\n",
       "      <th>GameId</th>\n",
       "      <th>PercentBet_ML</th>\n",
       "      <th>Open_Line_ML</th>\n",
       "      <th>Pinnacle_ML</th>\n",
       "      <th>5dimes_ML</th>\n",
       "      <th>Heritage_ML</th>\n",
       "      <th>Bovada_ML</th>\n",
       "      <th>Betonline_ML</th>\n",
       "      <th>Average_Line_ML</th>\n",
       "      <th>Best_Line_ML</th>\n",
       "      <th>Worst_Line_ML</th>\n",
       "      <th>PercentBet_Spread</th>\n",
       "      <th>Average_Line_Spread</th>\n",
       "      <th>Average_Odds_Spread</th>\n",
       "      <th>Best_Line_Spread</th>\n",
       "      <th>Worst_Line_Spread</th>\n",
       "      <th>Best_Odds_Spread</th>\n",
       "      <th>Worst_Odds_Spread</th>\n",
       "      <th>Average_Line_OU</th>\n",
       "      <th>Pts</th>\n",
       "      <th>Spread</th>\n",
       "      <th>Result</th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-10-30</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Washington</td>\n",
       "      <td>Cleveland</td>\n",
       "      <td>1610612764</td>\n",
       "      <td>21200001</td>\n",
       "      <td>-0.215798</td>\n",
       "      <td>0.210024</td>\n",
       "      <td>0.121038</td>\n",
       "      <td>0.086718</td>\n",
       "      <td>0.149203</td>\n",
       "      <td>0.037396</td>\n",
       "      <td>0.143883</td>\n",
       "      <td>2.090000</td>\n",
       "      <td>2.150000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-0.325510</td>\n",
       "      <td>0.814890</td>\n",
       "      <td>0.046142</td>\n",
       "      <td>0.729006</td>\n",
       "      <td>0.813276</td>\n",
       "      <td>-0.761344</td>\n",
       "      <td>0.768062</td>\n",
       "      <td>-1.340814</td>\n",
       "      <td>84.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>0</td>\n",
       "      <td>178.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-10-30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Cleveland</td>\n",
       "      <td>Washington</td>\n",
       "      <td>1610612739</td>\n",
       "      <td>21200001</td>\n",
       "      <td>0.247130</td>\n",
       "      <td>-0.532391</td>\n",
       "      <td>-0.508086</td>\n",
       "      <td>-0.520333</td>\n",
       "      <td>-0.532197</td>\n",
       "      <td>-0.268073</td>\n",
       "      <td>-0.517031</td>\n",
       "      <td>0.406504</td>\n",
       "      <td>0.425532</td>\n",
       "      <td>0.392157</td>\n",
       "      <td>0.342055</td>\n",
       "      <td>-0.814900</td>\n",
       "      <td>-1.188992</td>\n",
       "      <td>-0.813198</td>\n",
       "      <td>-0.729014</td>\n",
       "      <td>-0.761344</td>\n",
       "      <td>-0.868950</td>\n",
       "      <td>-1.340814</td>\n",
       "      <td>94.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>178.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-10-30</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Boston</td>\n",
       "      <td>Miami</td>\n",
       "      <td>1610612738</td>\n",
       "      <td>21200002</td>\n",
       "      <td>-0.207226</td>\n",
       "      <td>0.210024</td>\n",
       "      <td>0.248782</td>\n",
       "      <td>0.240078</td>\n",
       "      <td>0.229367</td>\n",
       "      <td>0.054774</td>\n",
       "      <td>0.241076</td>\n",
       "      <td>2.348000</td>\n",
       "      <td>2.440000</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>-0.099015</td>\n",
       "      <td>0.919722</td>\n",
       "      <td>-0.223379</td>\n",
       "      <td>0.939307</td>\n",
       "      <td>0.953484</td>\n",
       "      <td>1.280837</td>\n",
       "      <td>-0.868950</td>\n",
       "      <td>-1.649198</td>\n",
       "      <td>107.0</td>\n",
       "      <td>-13.0</td>\n",
       "      <td>0</td>\n",
       "      <td>227.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-10-30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Miami</td>\n",
       "      <td>Boston</td>\n",
       "      <td>1610612748</td>\n",
       "      <td>21200002</td>\n",
       "      <td>0.238557</td>\n",
       "      <td>-0.532391</td>\n",
       "      <td>-0.531342</td>\n",
       "      <td>-0.532622</td>\n",
       "      <td>-0.544073</td>\n",
       "      <td>-0.270645</td>\n",
       "      <td>-0.533693</td>\n",
       "      <td>0.370370</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.115560</td>\n",
       "      <td>-0.919732</td>\n",
       "      <td>-0.045775</td>\n",
       "      <td>-0.953398</td>\n",
       "      <td>-0.939327</td>\n",
       "      <td>-0.761344</td>\n",
       "      <td>0.768062</td>\n",
       "      <td>-1.649198</td>\n",
       "      <td>120.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1</td>\n",
       "      <td>227.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-10-30</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>L.A. Lakers</td>\n",
       "      <td>1610612742</td>\n",
       "      <td>21200003</td>\n",
       "      <td>-0.378680</td>\n",
       "      <td>0.741670</td>\n",
       "      <td>0.808599</td>\n",
       "      <td>0.930199</td>\n",
       "      <td>0.910767</td>\n",
       "      <td>0.315436</td>\n",
       "      <td>0.824235</td>\n",
       "      <td>3.946000</td>\n",
       "      <td>4.200000</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>-0.051331</td>\n",
       "      <td>1.223037</td>\n",
       "      <td>0.499098</td>\n",
       "      <td>1.219707</td>\n",
       "      <td>1.304005</td>\n",
       "      <td>0.239919</td>\n",
       "      <td>0.768062</td>\n",
       "      <td>-1.696895</td>\n",
       "      <td>99.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "      <td>190.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17203</th>\n",
       "      <td>2019-04-10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Denver</td>\n",
       "      <td>Minnesota</td>\n",
       "      <td>1610612743</td>\n",
       "      <td>21801228</td>\n",
       "      <td>-0.190080</td>\n",
       "      <td>-0.644243</td>\n",
       "      <td>-0.620646</td>\n",
       "      <td>-0.633891</td>\n",
       "      <td>-0.648958</td>\n",
       "      <td>-0.318843</td>\n",
       "      <td>-0.629344</td>\n",
       "      <td>0.112511</td>\n",
       "      <td>0.125945</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.580471</td>\n",
       "      <td>-1.770969</td>\n",
       "      <td>0.225396</td>\n",
       "      <td>-1.794601</td>\n",
       "      <td>-1.710472</td>\n",
       "      <td>-0.761344</td>\n",
       "      <td>0.768062</td>\n",
       "      <td>0.969188</td>\n",
       "      <td>99.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>194.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17204</th>\n",
       "      <td>2019-04-10</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Utah</td>\n",
       "      <td>L.A. Clippers</td>\n",
       "      <td>1610612762</td>\n",
       "      <td>21801229</td>\n",
       "      <td>0.512884</td>\n",
       "      <td>0.124143</td>\n",
       "      <td>0.267567</td>\n",
       "      <td>0.374268</td>\n",
       "      <td>0.389697</td>\n",
       "      <td>0.132972</td>\n",
       "      <td>0.299392</td>\n",
       "      <td>2.628000</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>2.490000</td>\n",
       "      <td>-0.605648</td>\n",
       "      <td>1.024555</td>\n",
       "      <td>-0.314451</td>\n",
       "      <td>1.079507</td>\n",
       "      <td>0.673068</td>\n",
       "      <td>0.239919</td>\n",
       "      <td>-0.868950</td>\n",
       "      <td>1.551417</td>\n",
       "      <td>137.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>280.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17205</th>\n",
       "      <td>2019-04-10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>L.A. Clippers</td>\n",
       "      <td>Utah</td>\n",
       "      <td>1610612746</td>\n",
       "      <td>21801229</td>\n",
       "      <td>-0.481553</td>\n",
       "      <td>-0.519284</td>\n",
       "      <td>-0.534259</td>\n",
       "      <td>-0.554378</td>\n",
       "      <td>-0.571065</td>\n",
       "      <td>-0.285110</td>\n",
       "      <td>-0.542950</td>\n",
       "      <td>0.321337</td>\n",
       "      <td>0.355872</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.622193</td>\n",
       "      <td>-1.024564</td>\n",
       "      <td>-1.105846</td>\n",
       "      <td>-0.672998</td>\n",
       "      <td>-1.079535</td>\n",
       "      <td>-0.761344</td>\n",
       "      <td>-5.743062</td>\n",
       "      <td>1.551417</td>\n",
       "      <td>143.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>280.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17206</th>\n",
       "      <td>2019-04-10</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Sacramento</td>\n",
       "      <td>Portland</td>\n",
       "      <td>1610612758</td>\n",
       "      <td>21801230</td>\n",
       "      <td>0.362862</td>\n",
       "      <td>0.606714</td>\n",
       "      <td>-0.160749</td>\n",
       "      <td>-0.162493</td>\n",
       "      <td>-0.171456</td>\n",
       "      <td>-0.110313</td>\n",
       "      <td>-0.186574</td>\n",
       "      <td>1.310000</td>\n",
       "      <td>1.350000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>0.502985</td>\n",
       "      <td>0.512974</td>\n",
       "      <td>0.318169</td>\n",
       "      <td>0.869206</td>\n",
       "      <td>0.532860</td>\n",
       "      <td>-0.761344</td>\n",
       "      <td>0.768062</td>\n",
       "      <td>1.674771</td>\n",
       "      <td>131.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>267.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17207</th>\n",
       "      <td>2019-04-10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Portland</td>\n",
       "      <td>Sacramento</td>\n",
       "      <td>1610612757</td>\n",
       "      <td>21801230</td>\n",
       "      <td>-0.331530</td>\n",
       "      <td>-0.587443</td>\n",
       "      <td>-0.415807</td>\n",
       "      <td>-0.415669</td>\n",
       "      <td>-0.425311</td>\n",
       "      <td>-0.220370</td>\n",
       "      <td>-0.404421</td>\n",
       "      <td>0.676590</td>\n",
       "      <td>0.689655</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>-0.486440</td>\n",
       "      <td>-0.512983</td>\n",
       "      <td>0.318169</td>\n",
       "      <td>-0.532797</td>\n",
       "      <td>-0.869222</td>\n",
       "      <td>-0.761344</td>\n",
       "      <td>0.768062</td>\n",
       "      <td>1.674771</td>\n",
       "      <td>136.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>267.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17202 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date  Location           Team        OppTeam      TeamId  \\\n",
       "0      2012-10-30      -1.0     Washington      Cleveland  1610612764   \n",
       "1      2012-10-30       1.0      Cleveland     Washington  1610612739   \n",
       "2      2012-10-30      -1.0         Boston          Miami  1610612738   \n",
       "3      2012-10-30       1.0          Miami         Boston  1610612748   \n",
       "4      2012-10-30      -1.0         Dallas    L.A. Lakers  1610612742   \n",
       "...           ...       ...            ...            ...         ...   \n",
       "17203  2019-04-10       1.0         Denver      Minnesota  1610612743   \n",
       "17204  2019-04-10      -1.0           Utah  L.A. Clippers  1610612762   \n",
       "17205  2019-04-10       1.0  L.A. Clippers           Utah  1610612746   \n",
       "17206  2019-04-10      -1.0     Sacramento       Portland  1610612758   \n",
       "17207  2019-04-10       1.0       Portland     Sacramento  1610612757   \n",
       "\n",
       "         GameId  PercentBet_ML  Open_Line_ML  Pinnacle_ML  5dimes_ML  \\\n",
       "0      21200001      -0.215798      0.210024     0.121038   0.086718   \n",
       "1      21200001       0.247130     -0.532391    -0.508086  -0.520333   \n",
       "2      21200002      -0.207226      0.210024     0.248782   0.240078   \n",
       "3      21200002       0.238557     -0.532391    -0.531342  -0.532622   \n",
       "4      21200003      -0.378680      0.741670     0.808599   0.930199   \n",
       "...         ...            ...           ...          ...        ...   \n",
       "17203  21801228      -0.190080     -0.644243    -0.620646  -0.633891   \n",
       "17204  21801229       0.512884      0.124143     0.267567   0.374268   \n",
       "17205  21801229      -0.481553     -0.519284    -0.534259  -0.554378   \n",
       "17206  21801230       0.362862      0.606714    -0.160749  -0.162493   \n",
       "17207  21801230      -0.331530     -0.587443    -0.415807  -0.415669   \n",
       "\n",
       "       Heritage_ML  Bovada_ML  Betonline_ML  Average_Line_ML  Best_Line_ML  \\\n",
       "0         0.149203   0.037396      0.143883         2.090000      2.150000   \n",
       "1        -0.532197  -0.268073     -0.517031         0.406504      0.425532   \n",
       "2         0.229367   0.054774      0.241076         2.348000      2.440000   \n",
       "3        -0.544073  -0.270645     -0.533693         0.370370      0.384615   \n",
       "4         0.910767   0.315436      0.824235         3.946000      4.200000   \n",
       "...            ...        ...           ...              ...           ...   \n",
       "17203    -0.648958  -0.318843     -0.629344         0.112511      0.125945   \n",
       "17204     0.389697   0.132972      0.299392         2.628000      2.750000   \n",
       "17205    -0.571065  -0.285110     -0.542950         0.321337      0.355872   \n",
       "17206    -0.171456  -0.110313     -0.186574         1.310000      1.350000   \n",
       "17207    -0.425311  -0.220370     -0.404421         0.676590      0.689655   \n",
       "\n",
       "       Worst_Line_ML  PercentBet_Spread  Average_Line_Spread  \\\n",
       "0           2.000000          -0.325510             0.814890   \n",
       "1           0.392157           0.342055            -0.814900   \n",
       "2           2.250000          -0.099015             0.919722   \n",
       "3           0.357143           0.115560            -0.919732   \n",
       "4           3.750000          -0.051331             1.223037   \n",
       "...              ...                ...                  ...   \n",
       "17203       0.100000           0.580471            -1.770969   \n",
       "17204       2.490000          -0.605648             1.024555   \n",
       "17205       0.294118           0.622193            -1.024564   \n",
       "17206       1.250000           0.502985             0.512974   \n",
       "17207       0.666667          -0.486440            -0.512983   \n",
       "\n",
       "       Average_Odds_Spread  Best_Line_Spread  Worst_Line_Spread  \\\n",
       "0                 0.046142          0.729006           0.813276   \n",
       "1                -1.188992         -0.813198          -0.729014   \n",
       "2                -0.223379          0.939307           0.953484   \n",
       "3                -0.045775         -0.953398          -0.939327   \n",
       "4                 0.499098          1.219707           1.304005   \n",
       "...                    ...               ...                ...   \n",
       "17203             0.225396         -1.794601          -1.710472   \n",
       "17204            -0.314451          1.079507           0.673068   \n",
       "17205            -1.105846         -0.672998          -1.079535   \n",
       "17206             0.318169          0.869206           0.532860   \n",
       "17207             0.318169         -0.532797          -0.869222   \n",
       "\n",
       "       Best_Odds_Spread  Worst_Odds_Spread  Average_Line_OU    Pts  Spread  \\\n",
       "0             -0.761344           0.768062        -1.340814   84.0   -10.0   \n",
       "1             -0.761344          -0.868950        -1.340814   94.0    10.0   \n",
       "2              1.280837          -0.868950        -1.649198  107.0   -13.0   \n",
       "3             -0.761344           0.768062        -1.649198  120.0    13.0   \n",
       "4              0.239919           0.768062        -1.696895   99.0     8.0   \n",
       "...                 ...                ...              ...    ...     ...   \n",
       "17203         -0.761344           0.768062         0.969188   99.0     4.0   \n",
       "17204          0.239919          -0.868950         1.551417  137.0    -6.0   \n",
       "17205         -0.761344          -5.743062         1.551417  143.0     6.0   \n",
       "17206         -0.761344           0.768062         1.674771  131.0    -5.0   \n",
       "17207         -0.761344           0.768062         1.674771  136.0     5.0   \n",
       "\n",
       "      Result  Total  \n",
       "0          0  178.0  \n",
       "1          1  178.0  \n",
       "2          0  227.0  \n",
       "3          1  227.0  \n",
       "4          1  190.0  \n",
       "...      ...    ...  \n",
       "17203      1  194.0  \n",
       "17204      0  280.0  \n",
       "17205      1  280.0  \n",
       "17206      0  267.0  \n",
       "17207      1  267.0  \n",
       "\n",
       "[17202 rows x 28 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = odds_df\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2f8f80",
   "metadata": {},
   "source": [
    "## 2.2 Perform a train-dev-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f10774f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n,_ = data.shape\n",
    "\n",
    "train_dev_df, test_df = train_test_split(data, test_size=.1, random_state=4000)\n",
    "\n",
    "train_df, dev_df = train_test_split(train_dev_df, test_size=.1, random_state=4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1eb9f6b",
   "metadata": {},
   "source": [
    "### 2.3 Implement the 2-layer Nerual Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9cd034b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class basicMLP(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(basicMLP, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            #feature layer\n",
    "            nn.Linear(16,100),\n",
    "            nn.Tanh(),\n",
    "            #hidden layers\n",
    "            \n",
    "            nn.Linear(100,100),    \n",
    "            nn.Tanh(), \n",
    "            \n",
    "            nn.Linear(100,100),    \n",
    "            nn.Tanh(), \n",
    "            \n",
    "            #output layer\n",
    "            nn.Linear(100,2)\n",
    "            \n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86b0fee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wrapper(nn.Module):\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        super(Wrapper, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            model,\n",
    "            nn.Softmax(1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd4494ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OddsData(Dataset):\n",
    "    def __init__(self, df):\n",
    "        features = ['Location','PercentBet_ML', 'Open_Line_ML', 'Pinnacle_ML', '5dimes_ML',\n",
    "       'Heritage_ML', 'Bovada_ML', 'Betonline_ML', 'PercentBet_Spread',\n",
    "       'Average_Line_Spread', 'Average_Odds_Spread', 'Best_Line_Spread',\n",
    "       'Worst_Line_Spread', 'Best_Odds_Spread', 'Worst_Odds_Spread', \n",
    "       'Average_Line_OU']\n",
    "        self.data_df = df.reset_index()\n",
    "        \n",
    "        x = df.loc[:,features].values\n",
    "        y = df.loc[:,'Result'].values.astype(int)\n",
    "        \n",
    "        self.X = torch.tensor(x, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=int)\n",
    "        \n",
    "        #self.y = self.y.type(torch.LongTensor)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.X[idx] ,self.y[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8fb68ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = OddsData(train_df)\n",
    "dev_data   = OddsData(dev_df)\n",
    "test_data  = OddsData(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aab79370",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 64\n",
    "train_loader = DataLoader(train_data, batch_size=bs, shuffle=True, num_workers=0)\n",
    "dev_loader = DataLoader(dev_data, batch_size=bs, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_data, batch_size=bs, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e99051d",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {\n",
    "    0: 'L',\n",
    "    1: 'W'\n",
    "}\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd5e1d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, criterion, optimizer, trainloader):\n",
    "   \n",
    "    size = len(trainloader.dataset)\n",
    "    \n",
    "    for batchnum, (X,y) in enumerate(trainloader):\n",
    "        model.train()\n",
    "        \n",
    "        yhat = model(X)\n",
    "        loss = criterion(yhat, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9668b653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, criterion, valloader):\n",
    "    model.eval()\n",
    "    \n",
    "    size = len(valloader.dataset)\n",
    "    num_batches = len(valloader)\n",
    "    test_loss, correct = 0,0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, y in valloader:\n",
    "            yhat = model(X)\n",
    "            test_loss += criterion(yhat, y).item()\n",
    "            correct += ( yhat.argmax(1) == y).type(torch.float).sum().item()\n",
    "            \n",
    "    loss = test_loss / num_batches\n",
    "    accuracy = correct / size\n",
    "    \n",
    "    return (loss, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbae3e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = basicMLP().to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "82042867",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adagrad(model.parameters(), lr=.0001, weight_decay=.001)\n",
    "\n",
    "crit = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba6bef5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "138d05db",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10000\n",
    "error = np.empty((epochs,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bbe8338c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d563b1a154d54630806eab4428987d2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 68.0%, Avg loss: 0.622624 \n",
      "\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 0.580877 \n",
      "\n",
      "Epoch 201\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.580266 \n",
      "\n",
      "Epoch 301\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 0.579927 \n",
      "\n",
      "Epoch 401\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.4%, Avg loss: 0.579629 \n",
      "\n",
      "Epoch 501\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 0.579942 \n",
      "\n",
      "Epoch 601\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 0.580009 \n",
      "\n",
      "Epoch 701\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 0.579926 \n",
      "\n",
      "Epoch 801\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 0.579517 \n",
      "\n",
      "Epoch 901\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.3%, Avg loss: 0.579674 \n",
      "\n",
      "Epoch 1001\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.4%, Avg loss: 0.580009 \n",
      "\n",
      "Epoch 1101\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 0.579640 \n",
      "\n",
      "Epoch 1201\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.579809 \n",
      "\n",
      "Epoch 1301\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.579978 \n",
      "\n",
      "Epoch 1401\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.579355 \n",
      "\n",
      "Epoch 1501\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.579404 \n",
      "\n",
      "Epoch 1601\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 0.579992 \n",
      "\n",
      "Epoch 1701\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 0.579399 \n",
      "\n",
      "Epoch 1801\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.4%, Avg loss: 0.579525 \n",
      "\n",
      "Epoch 1901\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.4%, Avg loss: 0.579426 \n",
      "\n",
      "Epoch 2001\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.4%, Avg loss: 0.579202 \n",
      "\n",
      "Epoch 2101\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.4%, Avg loss: 0.579614 \n",
      "\n",
      "Epoch 2201\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.4%, Avg loss: 0.579490 \n",
      "\n",
      "Epoch 2301\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.4%, Avg loss: 0.579404 \n",
      "\n",
      "Epoch 2401\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 0.579217 \n",
      "\n",
      "Epoch 2501\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 0.578815 \n",
      "\n",
      "Epoch 2601\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 0.578846 \n",
      "\n",
      "Epoch 2701\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 0.579063 \n",
      "\n",
      "Epoch 2801\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 0.579287 \n",
      "\n",
      "Epoch 2901\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 0.579461 \n",
      "\n",
      "Epoch 3001\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 0.579269 \n",
      "\n",
      "Epoch 3101\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.578861 \n",
      "\n",
      "Epoch 3201\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.579157 \n",
      "\n",
      "Epoch 3301\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.579237 \n",
      "\n",
      "Epoch 3401\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.578945 \n",
      "\n",
      "Epoch 3501\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.579068 \n",
      "\n",
      "Epoch 3601\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.579271 \n",
      "\n",
      "Epoch 3701\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 0.578916 \n",
      "\n",
      "Epoch 3801\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 0.578731 \n",
      "\n",
      "Epoch 3901\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 0.578847 \n",
      "\n",
      "Epoch 4001\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.5%, Avg loss: 0.578807 \n",
      "\n",
      "Epoch 4101\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.579232 \n",
      "\n",
      "Epoch 4201\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.579098 \n",
      "\n",
      "Epoch 4301\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.579003 \n",
      "\n",
      "Epoch 4401\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.578685 \n",
      "\n",
      "Epoch 4501\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.578929 \n",
      "\n",
      "Epoch 4601\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.578834 \n",
      "\n",
      "Epoch 4701\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.578678 \n",
      "\n",
      "Epoch 4801\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.578603 \n",
      "\n",
      "Epoch 4901\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.579304 \n",
      "\n",
      "Epoch 5001\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.578593 \n",
      "\n",
      "Epoch 5101\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.578753 \n",
      "\n",
      "Epoch 5201\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.578878 \n",
      "\n",
      "Epoch 5301\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.578736 \n",
      "\n",
      "Epoch 5401\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.578973 \n",
      "\n",
      "Epoch 5501\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.578605 \n",
      "\n",
      "Epoch 5601\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.578862 \n",
      "\n",
      "Epoch 5701\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.578831 \n",
      "\n",
      "Epoch 5801\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.578464 \n",
      "\n",
      "Epoch 5901\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.578638 \n",
      "\n",
      "Epoch 6001\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.578448 \n",
      "\n",
      "Epoch 6101\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.578417 \n",
      "\n",
      "Epoch 6201\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.579213 \n",
      "\n",
      "Epoch 6301\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.578727 \n",
      "\n",
      "Epoch 6401\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.578602 \n",
      "\n",
      "Epoch 6501\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.578814 \n",
      "\n",
      "Epoch 6601\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.579053 \n",
      "\n",
      "Epoch 6701\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.578521 \n",
      "\n",
      "Epoch 6801\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.579032 \n",
      "\n",
      "Epoch 6901\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.578640 \n",
      "\n",
      "Epoch 7001\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.578301 \n",
      "\n",
      "Epoch 7101\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.578505 \n",
      "\n",
      "Epoch 7201\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.578803 \n",
      "\n",
      "Epoch 7301\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.578664 \n",
      "\n",
      "Epoch 7401\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.579024 \n",
      "\n",
      "Epoch 7501\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.578825 \n",
      "\n",
      "Epoch 7601\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.578917 \n",
      "\n",
      "Epoch 7701\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.578682 \n",
      "\n",
      "Epoch 7801\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.579034 \n",
      "\n",
      "Epoch 7901\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.578761 \n",
      "\n",
      "Epoch 8001\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.578490 \n",
      "\n",
      "Epoch 8101\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.578754 \n",
      "\n",
      "Epoch 8201\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.579090 \n",
      "\n",
      "Epoch 8301\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.578341 \n",
      "\n",
      "Epoch 8401\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.578818 \n",
      "\n",
      "Epoch 8501\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.578737 \n",
      "\n",
      "Epoch 8601\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.578433 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8701\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.578422 \n",
      "\n",
      "Epoch 8801\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.578407 \n",
      "\n",
      "Epoch 8901\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.578989 \n",
      "\n",
      "Epoch 9001\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.578714 \n",
      "\n",
      "Epoch 9101\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.578445 \n",
      "\n",
      "Epoch 9201\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.578790 \n",
      "\n",
      "Epoch 9301\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.578487 \n",
      "\n",
      "Epoch 9401\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.578583 \n",
      "\n",
      "Epoch 9501\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.578217 \n",
      "\n",
      "Epoch 9601\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.578324 \n",
      "\n",
      "Epoch 9701\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.578496 \n",
      "\n",
      "Epoch 9801\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.578375 \n",
      "\n",
      "Epoch 9901\n",
      "-------------------------------\n",
      "Test Error: \n",
      " Accuracy: 69.6%, Avg loss: 0.578112 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(epochs)):\n",
    "    \n",
    "    train_epoch(model, crit, optimizer, train_loader)\n",
    "    \n",
    "    loss, accuracy = validate(model, crit, test_loader )\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "        print(f\"Test Error: \\n Accuracy: {(accuracy*100):>0.1f}%, Avg loss: {loss:>8f} \\n\")\n",
    "    error[epoch] = [accuracy,loss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a0a8415f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0.5869111595077252, 0.6880562733275911),\n",
       " (0.6051433753967285, 0.6623628147191737),\n",
       " (0.5786497052069064, 0.6955258570598489))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate(model, crit, train_loader), validate(model, crit, dev_loader), validate(model, crit, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8f2bef29",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrap = Wrapper(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "70adab54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36.78974609569066, 39.0, 0.9433268229664272)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "winnings, bet, _ = evaluate(wrap,dev_data, min_dif=.047, standardize=False)\n",
    "winnings.sum(), bet.sum(), winnings.sum()/ bet.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f58bd3c",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "The basic accuracy here is not that relevant to us. What we care about is how the model performs against the odds. For this purpose we use our model to select which bets are 'good'. I.e. the model predicts a higher probability of success than the implied odds. Hence a positive EV proposition. Because of the Vig, a model the perfectly mimicks the books implied odds will lose ~%5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0eb3632f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def implied_probability(frac):\n",
    "    return 1/(1+frac)\n",
    "def to_frac(lines):\n",
    "    cond = odds_df[lines] > 0\n",
    "    pos = (odds_df[lines]/100)\n",
    "    neg = (-100/odds_df[lines])\n",
    "\n",
    "    return pos.where(cond, other=neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fc2c444e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data, min_dif=0, standardize=False):\n",
    "    imp_prob = implied_probability(data.data_df['Best_Line_ML'])\n",
    "    yhat_prob = model(data.X)[:,1]\n",
    "    if type(yhat_prob) is torch.Tensor:\n",
    "        yhat_prob = yhat_prob.detach().numpy() #predicted probability of Win\n",
    "    \n",
    "    dif = yhat_prob - imp_prob\n",
    "    good_bets_idx = dif[dif > min_dif].index\n",
    "    n = len(good_bets_idx)\n",
    "    if standardize:\n",
    "        bet = 1/(data.data_df.loc[good_bets_idx]['Best_Line_ML']+1)\n",
    "    else:\n",
    "        bet = np.ones(n)\n",
    "    winnings = (data.data_df.loc[good_bets_idx]['Best_Line_ML']+1) * bet * data.y[good_bets_idx].detach().numpy()\n",
    "    \n",
    "    return (winnings, bet,good_bets_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "59f41f9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(591.7779334027127, 632.0, 0.9363574895612543)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "winnings, bet, gbi = evaluate(wrap, test_data, min_dif=0.0, standardize=False)\n",
    "winnings.sum(), sum(bet), (winnings.sum()) / sum(bet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ce93d1b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(69.0, 68.94594457911502, 1.0007840261122676)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "winnings, bet, gbi = evaluate(wrap, test_data, min_dif=0.03, standardize=True)\n",
    "winnings.sum(), sum(bet), (winnings.sum()) / sum(bet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5da507a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc0821d",
   "metadata": {},
   "source": [
    "# Task 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee28da2",
   "metadata": {},
   "source": [
    "The number of nodes in the hidden layers being set at 100 is faily arbitrary. The use of Softmax activation function in the final layer of the Wrapper is because we want to output probabilities. I did use regulariation because the first couple times I trained the model the training accuracy was much higher than the test accuracy, hence it was overfitting. I used Adagrad optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b597dd7",
   "metadata": {},
   "source": [
    "# Task 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "97ea522a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(train_data.X, train_data.y)\n",
    "yhat = lr_model.predict(test_data.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "362f6071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6984311446833237"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(test_data.y, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "68b57e8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(619.7721722836905, 631.0, 0.9822062952197947)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "winnings, bet, gbi = evaluate(lr_model.predict_proba, test_data, min_dif=0.0, standardize=False)\n",
    "winnings.sum(), sum(bet), (winnings.sum()) / sum(bet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f5fe3fde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35.0, 35.34780093559485, 0.9901606061370394)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "winnings, bet, gbi = evaluate(lr_model.predict_proba, test_data, min_dif=0.03, standardize=True)\n",
    "winnings.sum(), sum(bet), (winnings.sum()) / sum(bet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4ecc6340",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_yhat = np.where(test_data.data_df['Best_Line_ML'] < 1,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e545c4b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6995932597327136"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(test_data.y, book_yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8fad20",
   "metadata": {},
   "source": [
    "We compare the model to a baseline LogisticRegression model. The LR model performs better or comparable to the the neural network. This is somewhat surpising to me as the NN does not appear to be overfitting. What it really goes to show is that you really can't beat the book."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
