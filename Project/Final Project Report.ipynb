{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d2c74c8",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Dynamic Mode Decomposition (DMD) is a method, first published by Peter Schmid in 2010, that models the evolution of a system over time [1]. While first developed as a means of modeling dynamical systems such as fluid flows and other physical states, its applications have been expanded. Analyzing training a Neural Network as a dynamical system has been done previously[2] [3]. This work, specifically, investigates the application of DMD in accelerating the training of Graph Neural Networks. While similar to traditional feedforward networks GNNs differ in ways that may alter the appropriateness of various optimization algorithms. Also central to this work is the use of coarsening techniques on the graph data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3d9e84",
   "metadata": {},
   "source": [
    "## Literature Review\n",
    "\n",
    "In preparing for this work I reviewed multiple papers to learn about DMD and its application to accelerating training in artificial neural networks. I also familiarized myself with GNNs using a variety of resources. TensorFlow’s Machine Learning Tech Talks on GNNs by Senior Research Scientist at DeepMind, Petar Veličković, was particularly nice and I would recommend it to anyone looking to inquire about GNNs. The main papers I reviewed were [2], [3], and [4].\n",
    "\n",
    "*In the case of [3], which totals 110 pages, ‘read’ is used to mean the abstract plus skimming some other sections. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7309167",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8b889c",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "The model extends the pytorch nn.Module class and is implemented using PyTorch Geometric convolutional layers. The model has two convolutional layers, one dropout layer, and a dense Linear layer. Since this is a classification problem, softmax activation is used as the final layer. The model takes in three hyperparamaters: the number of features that each vertex has, the size of the convolutional layer, and the number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa6f7261",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,num_features, hidden, out_features):\n",
    "        super(GNN,self).__init__()\n",
    "        seed = np.random.randint(0,high=999999,dtype=int)\n",
    "        torch.manual_seed(seed)\n",
    "        self.conv1 = GCNConv(num_features,hidden)\n",
    "        self.conv2 = GCNConv(hidden,hidden)\n",
    "        self.out_features = out_features\n",
    "        self.end_layer = nn.Linear(hidden, out_features)\n",
    "    def reset_parameters(self):\n",
    "        self.conv1.reset_parameters()\n",
    "        self.conv2.reset_parameters()\n",
    "        self.end_layer.reset_parameters()\n",
    "    def forward(self,torchG):\n",
    "\n",
    "        x, edge_index = torchG.x, torchG.edge_index\n",
    "        x = x.float()\n",
    "\n",
    "        x = self.conv1(x,edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x,training=self.training)\n",
    "        x = self.conv2(x,edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.end_layer(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bdf401",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52e4a235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_level(model, TGG_D, optimizer, loss_fn, level, m=None, pred_step=None, r=2, epochs=20):\n",
    "    \"\"\"    \n",
    "    model: model to be trained\n",
    "    TGG_D: Torch Geometric Graph Dictionary. Each TGG represents a different level of coarsening.\n",
    "    optimizer: torch.optim optimizer used for training.\n",
    "    loss_fn: torch loss function.\n",
    "    level: The level of coarsening to perform training on. \n",
    "    epochs: total number of epochs to perform training.\n",
    "        following arguments are supplied if DMD is used:\n",
    "    m: DMDstep is performed after m epochs\n",
    "    pred_step: how many timesteps to step forward.\n",
    "    r: number of modes computed in DMD\"\"\"\n",
    "    accuracy = []\n",
    "    model.train()\n",
    "    if m is None:\n",
    "        m = epochs+1\n",
    "    data = TGG_D[level].to(device)\n",
    "    data.x = F.normalize(data.x,p=1)\n",
    "    \n",
    "    weights = [ np.empty(np.append(1, param.shape)) for param in model.parameters()]\n",
    "    for i,param in enumerate(model.parameters()):\n",
    "        weights[i][0] = param.detach()\n",
    "\n",
    "\n",
    "    for epoch in tqdm(range(1,epochs+1)):\n",
    "        if epoch % m == 0:\n",
    "            print(\"DMD\")\n",
    "            DMDstep(model, weights, r, pred_step, params=None)\n",
    "            weights = [ np.empty(np.append(1, param.shape)) for param in model.parameters()]\n",
    "        train_epoch(model, data, optimizer, loss_fn)\n",
    "        for i,param  in enumerate(model.parameters()):\n",
    "            weights[i] = np.append(weights[i], param.detach().reshape(np.append(1,weights[i].shape[1:]).tolist()),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b356b8b",
   "metadata": {},
   "source": [
    "We can call train_level in a variety of fashions. The one that I have found to be the most successful is perform 2 iterations of train_level per level. 1 with DMD and then one without DMD. We then iterate through each level starting with the coarsest and back to the original graph. Because of computational limitations we cannot train for as many epochs on the original graph as the coarsen graphs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf73100",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46f9737",
   "metadata": {},
   "source": [
    "The results are promising but have not yet shown anything to the extent of which prior papers have shown with respect to accelerations in training times. When limiting training to 30 seconds per level and 4 levels of training with DMD achieved an average accuracy of 0.8508. Without using DMD accuracy was very slightly lower: 0.8488."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9415d472",
   "metadata": {},
   "source": [
    "These results are very sensitive to hyperparameter tuning and I suspect there is some configuration that is well suited to a particular class of problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76a669c",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "All of the data used in this work came from the Torch Geometric Datasets class. The class contains a plethora of graph datasets. We focused on the ones designed for vertex classification. The datasets in particular we examined were from the Planetoid subclass. This dataset contains three graphs where vertecies represent papers and edges represent citations. The features associated with each dataset are a Word2vec embedding for words contained in the paper.\n",
    "\n",
    "### r/Place\n",
    "\n",
    "r/Place was a collaboratize conducted by reddit in 2017 and 2022. In the project online users could place a single pixel on a public canvas up to once every 10 minutes. The data was subsequently released by Reddit. This data can be used to realize a graph. One such way is for vertices to represent users, with edges linked users who collaborated on the same square. Alternatively, vertices can represent squares which are connected by users. Unfortunately I did not have time to implement anything with this data as it is a unsupervised learning problem and required additional work beyond the main scope of the project. However, I did perform data processing which can be found in place_data_processing.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cda4f7",
   "metadata": {},
   "source": [
    "## Future Work\n",
    "\n",
    "The first step is to conduct hyperparameter optimization. If r is selected too large DMD takes too long. If r is too small DMD provides by results. This is just one example. A good hueristic is needed for hyperparameter selection. Once this is all complete in Python this all needs to be implemented in c. This will allow greater parralelization and will be where we can really speed up the expensive matrix operations needed to perform DMD."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7ac9d5",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. \n",
    "SCHMID, P. (2010). Dynamic mode decomposition of numerical and experimental data. Journal of Fluid Mechanics, 656, 5-28. doi:10.1017/S0022112010001217\n",
    "\n",
    "2. Tano, Mauricio E. and Portwood, Gavin D. and Ragusa, Jean C. (2020) Accelerating Training in Artificial Neural Networks with Dynamic Mode Decomposition\n",
    "\n",
    "3. Brunton, S.L., Budisic, M., Kaiser, E., & Kutz, J.N. (2021). Modern Koopman Theory for Dynamical Systems. SIAM Rev., 64, 229-340.\n",
    "\n",
    "4. Akshunna S. Dogra and William T. Redman. 2020. Optimizing neural networks via koopman operator theory. In Proceedings of the 34th International Conference on Neural Information Processing Systems (NIPS'20). Curran Associates Inc., Red Hook, NY, USA, Article 176, 2087–2097.\n",
    "\n",
    "5. Machine Learning Tech Talks, Senior Research Scientist at DeepMind, Petar Veličković"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
