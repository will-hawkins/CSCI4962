{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b054de9",
   "metadata": {},
   "source": [
    "# Task 1:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8cc88e",
   "metadata": {},
   "source": [
    "One application of a Markov Decision Process would be learning to drive a car. The state space would be the carâ€™s position, its velocity, and the environment. The action would be what acceleration (direction and magnitude) to apply. The rewards would be determined by whether the car drives cleanly, avoid all obstacles, stays within the lines of the road, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a5a1b9",
   "metadata": {},
   "source": [
    "# Task 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eabcbdb",
   "metadata": {},
   "source": [
    "One task where Reinforcement Learning is used is stock trading.  RL models can be trained to determine when to buy or when to sell a stock. The abundance of available data makes this a good task for RL. There are tens of thousands of public stocks. There exists easily accessible data for all stocks listed on the major exchanges going back to the 1980s at a variety of resolutions. Additionally, stock prices over time are traditionally modeled as being independent of previous values, which makes this a good problem to be modelled by a Markov Decision Process. \n",
    "\n",
    "Trading-bot is a project from pskrunner14 that uses Deep Q-Learning to train a model to make money in the stock market. The model is fairly simple. The state space is the stock price over the last n-days. The action space is whether to buy, sell, or hold the stock. The reward calculation is simply the change in the value of the portfolio from the action taken.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b30baa",
   "metadata": {},
   "source": [
    "# Task 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5148c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "from torch import Tensor\n",
    "import gymnasium as gym\n",
    "from tqdm.notebook import tqdm\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fabd6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self.board = torch.zeros((3,3))\n",
    "        self.turnX = True\n",
    "        self.moves = 0\n",
    "    def placeX(self,x,y):\n",
    "        if (self.board[x,y] != 0):\n",
    "            return False\n",
    "        self.board[x,y] = 1\n",
    "        self.turnX = False\n",
    "        self.moves+= 1\n",
    "        return True\n",
    "    def placeO(self,x,y):\n",
    "        if (self.board[x,y] != 0):\n",
    "            return False\n",
    "        self.board[x,y] = -1\n",
    "        self.turnX = True\n",
    "        self.moves+= 1\n",
    "        return True\n",
    "    def place(self, x,y):\n",
    "        if self.turnX:\n",
    "            return self.placeX(x,y)\n",
    "        else:\n",
    "            return self.placeO(x,y)\n",
    "    def checkWin(self):\n",
    "        \n",
    "        rows = self.board.sum(axis=0)\n",
    "        cols = self.board.sum(axis=1)\n",
    "        \n",
    "        if (3 in rows or 3 in cols or \n",
    "            3 in [self.board[0,0]+self.board[1,1]+self.board[2,2], self.board[0,2]+self.board[1,1]+self.board[2,0]] ):\n",
    "            return 1\n",
    "        elif (-3 in rows or -3 in cols or \n",
    "            -3 in [self.board[0,0]+self.board[1,1]+self.board[2,2], self.board[0,2]+self.board[1,1]+self.board[2,0]] ):\n",
    "            return -1\n",
    "        if self.moves == 9:\n",
    "            return 0\n",
    "        return None\n",
    "    def __str__(self):\n",
    "        return np.where(self.board>0,'X',np.where(self.board<0,'O','-')).__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "12794813",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, game,player):\n",
    "        self.game = game\n",
    "        if player in ['X', 1]:\n",
    "            self.player = 1\n",
    "        elif player in ['O', -1]:\n",
    "            self.player = -1\n",
    "        else:\n",
    "            assert(False)\n",
    "    def make_random_move(self):\n",
    "        x,y = divmod(self.random_move(),3)\n",
    "        return self.game.place(x,y)\n",
    "    def random_move(self):\n",
    "        return np.random.choice(torch.where(self.game.board.flatten() == 0)[0])\n",
    "    def move(self,action):\n",
    "        x,y = divmod(action,3)\n",
    "        return self.game.place(x,y)  \n",
    "    def illegal(self,action):\n",
    "        if self.game.board[action[0],action[1]] != 0:\n",
    "            return True\n",
    "        return False\n",
    "    def Flatten(self):\n",
    "        return self.game.board.flatten()\n",
    "    def state(self):\n",
    "        return self.game.board.reshape([1,3,3]).clone()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23d406a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d044549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "class DQN(nn.Module):\n",
    "    \n",
    "    def __init__(self, h,w, outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.stack = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=3,out_channels=20,kernel_size=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(20, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,32), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,outputs),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        state = x.reshape([x.shape[0],9])\n",
    "        logits = self.stack(x)\n",
    "        return (state**2-1)*-1*logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2626b109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(agent, model, EPS_END, EPS_START, steps_done, EPS_DECAY):\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    if random.random() > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return model(agent.state()).argmax().item()\n",
    "    return agent.random_move()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "44ee82bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(policy_net, target_net, trainer='random',memory=None, num_games=10000,\n",
    "                loss_reward=-1,win_reward=1,move_reward=.1,tie_reward=0,\n",
    "               BATCH_SIZE=128, GAMMA=0.999, EPS_START=.95, EPS_END=.05,EPS_DECAY=200, TARGET_UPDATE=10,\n",
    "               device='cpu', mem_len=10000, player='X'):\n",
    "    \n",
    "    #global agent_opp\n",
    "    agent_opp = None\n",
    "    \n",
    "    if trainer == 'random':\n",
    "        model_opp = lambda x: agent_opp.random_move()\n",
    "    elif type(trainer) == DQN:\n",
    "        model_opp = lambda x: trainer(x).argmax().item()\n",
    "    \n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "    target_net.eval()\n",
    "    steps_done = 0\n",
    "    opponent = None\n",
    "    if player in ['O',-1]:\n",
    "        player = -1\n",
    "        opponent = 1 \n",
    "        \n",
    "    elif player in ['X',1]:\n",
    "        player = 1\n",
    "        opponent = -1\n",
    "    else:\n",
    "        assert(False)\n",
    "        \n",
    "    optimizer = optim.RMSprop(policy_net.parameters())\n",
    "    if memory is None:\n",
    "        memory = ReplayMemory(mem_len)\n",
    "    \n",
    "    for i in tqdm(range(int(num_games))):\n",
    "        steps_done += 1\n",
    "        game = TicTacToe()\n",
    "        agent = Agent(game, player)\n",
    "        agent_opp = Agent(game, opponent)\n",
    "        \n",
    "        #if playing as O, opponent goes first\n",
    "        if agent.player == -1:\n",
    "            agent_opp.move(model_opp(agent_opp.state()))\n",
    "\n",
    "        for t in count():\n",
    "            state = agent.state()\n",
    "            action = select_action(agent, policy_net, EPS_END, EPS_START, steps_done, EPS_DECAY)\n",
    "\n",
    "            if agent.move(action):\n",
    "                w = game.checkWin()\n",
    "                if w == agent.player:\n",
    "                    reward = win_reward\n",
    "                    done = True\n",
    "                elif w == 0:\n",
    "                    reward = tie_reward\n",
    "                    done = True\n",
    "                else:\n",
    "                    agent_opp.move(model_opp(agent_opp.state()))\n",
    "                    w = game.checkWin()\n",
    "                    if w == agent_opp.player:\n",
    "                        reward = loss_reward\n",
    "                        done = True\n",
    "                    elif w == 0:\n",
    "                        reward = tie_reward\n",
    "                        done = True\n",
    "                    #legal move, game did not end\n",
    "                    else:\n",
    "                        reward = move_reward\n",
    "                        done = False\n",
    "            else:\n",
    "                reward = -10\n",
    "                done = False\n",
    "\n",
    "\n",
    "\n",
    "            next_state = agent.state()\n",
    "            memory.push(state, action, next_state, reward)\n",
    "\n",
    "            #Train model\n",
    "\n",
    "            if len(memory) >= BATCH_SIZE:\n",
    "                transitions = memory.sample(BATCH_SIZE)\n",
    "                batch = Transition(*zip(*transitions))\n",
    "                non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)),\n",
    "                                             device=device, dtype=torch.bool)\n",
    "                non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "\n",
    "                state_batch = torch.cat(batch.state)\n",
    "                action_batch = torch.tensor(batch.action).reshape([len(batch.action),1])\n",
    "                reward_batch = torch.tensor(batch.reward)\n",
    "\n",
    "                state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "                next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "                next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "                expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "                criterion = nn.SmoothL1Loss()\n",
    "                loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                for param in policy_net.parameters():\n",
    "                    param.data.clamp(-1,1)\n",
    "                optimizer.step()\n",
    "            if done:\n",
    "                break\n",
    "                print('done')\n",
    "\n",
    "            if t % TARGET_UPDATE == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "            \n",
    "    return policy_net, target_net, memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "f8260a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_X, model_Xtarget = DQN(3,3,9).to('cpu'), DQN(3,3,9).to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "95a704d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_O, model_Otarget = DQN(3,3,9).to('cpu'), DQN(3,3,9).to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "f2a1263f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "743509d214ea4cb7a71f74be833f1495",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(DQN(\n",
       "   (stack): Sequential(\n",
       "     (0): Conv1d(3, 20, kernel_size=(2,), stride=(1,))\n",
       "     (1): ReLU()\n",
       "     (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "     (3): Flatten(start_dim=1, end_dim=-1)\n",
       "     (4): Linear(in_features=20, out_features=32, bias=True)\n",
       "     (5): ReLU()\n",
       "     (6): Linear(in_features=32, out_features=64, bias=True)\n",
       "     (7): ReLU()\n",
       "     (8): Linear(in_features=64, out_features=32, bias=True)\n",
       "     (9): ReLU()\n",
       "     (10): Linear(in_features=32, out_features=9, bias=True)\n",
       "     (11): Softmax(dim=1)\n",
       "   )\n",
       " ),\n",
       " DQN(\n",
       "   (stack): Sequential(\n",
       "     (0): Conv1d(3, 20, kernel_size=(2,), stride=(1,))\n",
       "     (1): ReLU()\n",
       "     (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "     (3): Flatten(start_dim=1, end_dim=-1)\n",
       "     (4): Linear(in_features=20, out_features=32, bias=True)\n",
       "     (5): ReLU()\n",
       "     (6): Linear(in_features=32, out_features=64, bias=True)\n",
       "     (7): ReLU()\n",
       "     (8): Linear(in_features=64, out_features=32, bias=True)\n",
       "     (9): ReLU()\n",
       "     (10): Linear(in_features=32, out_features=9, bias=True)\n",
       "     (11): Softmax(dim=1)\n",
       "   )\n",
       " ),\n",
       " <__main__.ReplayMemory at 0x1ba33ab8280>)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(model_X, model_Xtarget, num_games=500, player='X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "2ef05f60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a6a68441b3f4266a895c60328b5d222",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(DQN(\n",
       "   (stack): Sequential(\n",
       "     (0): Conv1d(3, 20, kernel_size=(2,), stride=(1,))\n",
       "     (1): ReLU()\n",
       "     (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "     (3): Flatten(start_dim=1, end_dim=-1)\n",
       "     (4): Linear(in_features=20, out_features=32, bias=True)\n",
       "     (5): ReLU()\n",
       "     (6): Linear(in_features=32, out_features=64, bias=True)\n",
       "     (7): ReLU()\n",
       "     (8): Linear(in_features=64, out_features=32, bias=True)\n",
       "     (9): ReLU()\n",
       "     (10): Linear(in_features=32, out_features=9, bias=True)\n",
       "     (11): Softmax(dim=1)\n",
       "   )\n",
       " ),\n",
       " DQN(\n",
       "   (stack): Sequential(\n",
       "     (0): Conv1d(3, 20, kernel_size=(2,), stride=(1,))\n",
       "     (1): ReLU()\n",
       "     (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "     (3): Flatten(start_dim=1, end_dim=-1)\n",
       "     (4): Linear(in_features=20, out_features=32, bias=True)\n",
       "     (5): ReLU()\n",
       "     (6): Linear(in_features=32, out_features=64, bias=True)\n",
       "     (7): ReLU()\n",
       "     (8): Linear(in_features=64, out_features=32, bias=True)\n",
       "     (9): ReLU()\n",
       "     (10): Linear(in_features=32, out_features=9, bias=True)\n",
       "     (11): Softmax(dim=1)\n",
       "   )\n",
       " ),\n",
       " <__main__.ReplayMemory at 0x1ba339c3d90>)"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(model_O, model_Otarget, num_games=500, player='O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "79e9cd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim(game, modelX, agentX, modelO, agentO,verbose=False ):\n",
    "    \"\"\"\n",
    "    Returns 1 if X wins\n",
    "            -1 if O wins\n",
    "            0 if a tie\n",
    "    \"\"\"\n",
    "    w = game.checkWin()\n",
    "    while w == None:\n",
    "        action = modelX(agentX.state())\n",
    "        agentX.move(action)\n",
    "        w = game.checkWin()\n",
    "        if verbose:\n",
    "            print(game)\n",
    "            print()\n",
    "        if w is not None:\n",
    "            return w\n",
    "        action = modelO(agentO.state())\n",
    "        agentO.move(action)\n",
    "        w = game.checkWin()\n",
    "        if verbose:\n",
    "            print(game)\n",
    "            print()\n",
    "        if w is not None:\n",
    "            return w\n",
    "        \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "d89e3f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, opp_model, player='X',N=1000, verbose=False):\n",
    "    \"\"\"\n",
    "    Returns share of results for model\n",
    "    [Loss, Tie, Win]\n",
    "    \"\"\"\n",
    "    agent1, agent2 = None, None\n",
    "    \n",
    "    if model == 'random':\n",
    "        strat = lambda x: agent1.random_move()\n",
    "    elif type(model) == DQN:\n",
    "        strat = lambda x: model(x).argmax().item()\n",
    "    \n",
    "    if opp_model == 'random':\n",
    "        strat2 = lambda x: agent2.random_move()\n",
    "    elif type(opp_model) == DQN:\n",
    "        strat2 = lambda x: opp_model(x).argmax().item()\n",
    "        \n",
    "        \n",
    "    opponent = 'X' if player == 'O' else 'O'\n",
    "    results = np.empty(N)\n",
    "    if player == 'X':\n",
    "        for i in range(N):\n",
    "            game = TicTacToe()\n",
    "            agent1 = Agent(game, 'X')\n",
    "            agent2 = Agent(game, 'O')\n",
    "            results[i] = sim(game, strat, agent1, strat2, agent2,verbose=verbose)\n",
    "    elif player == 'O':\n",
    "        for i in range(N):\n",
    "            \n",
    "            game = TicTacToe()\n",
    "            agent1 = Agent(game, 'X')\n",
    "            agent2 = Agent(game, 'O')\n",
    "            results[i] = sim(game, strat2, agent2, strat, agent1, verbose=verbose) * -1\n",
    "    else:\n",
    "        assert(False)\n",
    "    v, c = np.unique(results, return_counts=True)\n",
    "    stats = np.zeros([3])\n",
    "    for i,j in zip(v,c):\n",
    "        stats[int(i+1)] = j\n",
    "    return stats /N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "42eec457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.253, 0.126, 0.621])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats0 = validate('random', opp_model='random', player='X')\n",
    "stats0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "b1f89ad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.102, 0.034, 0.864])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats1 = validate(model_X, opp_model='random', player='X')\n",
    "stats1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "7ead8c08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.215, 0.072, 0.713])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats2 = validate(model_O, opp_model='random', player='O')\n",
    "stats2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e0669a",
   "metadata": {},
   "source": [
    "### The model's playing as the other side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "38e4d10d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.576, 0.135, 0.289])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats3 = validate(model_X, opp_model='random', player='O')\n",
    "stats3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "6f8c470d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.179, 0.129, 0.692])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats4 = validate(model_O, opp_model='random', player='X')\n",
    "stats4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a803be",
   "metadata": {},
   "source": [
    "### The models playing against each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "5fe43d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['-' '-' '-']\n",
      " ['X' '-' '-']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "[['-' 'O' '-']\n",
      " ['X' '-' '-']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "[['-' 'O' '-']\n",
      " ['X' '-' 'X']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "[['-' 'O' 'O']\n",
      " ['X' '-' 'X']\n",
      " ['-' '-' '-']]\n",
      "\n",
      "[['-' 'O' 'O']\n",
      " ['X' '-' 'X']\n",
      " ['X' '-' '-']]\n",
      "\n",
      "[['-' 'O' 'O']\n",
      " ['X' 'O' 'X']\n",
      " ['X' '-' '-']]\n",
      "\n",
      "[['-' 'O' 'O']\n",
      " ['X' 'O' 'X']\n",
      " ['X' '-' 'X']]\n",
      "\n",
      "[['O' 'O' 'O']\n",
      " ['X' 'O' 'X']\n",
      " ['X' '-' 'X']]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0.])"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats5 = validate(model_X, opp_model=model_O, player='X',N=1, verbose=True)\n",
    "stats5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "fda0e653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Loss    Tie    Win\n",
      "--  ------  -----  -----\n",
      " 1   0.27   0.091  0.639\n",
      " 2   0.212  0.066  0.722\n",
      " 3   0.531  0.187  0.282\n",
      " 4   0.159  0.19   0.651\n",
      " 5   0      0      1\n"
     ]
    }
   ],
   "source": [
    "print(tabulate([stats1,stats2,stats3,stats4,stats5], showindex=[1,2,3,4,5], headers=['Loss','Tie','Win']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34d2c98",
   "metadata": {},
   "source": [
    "## Train Models against each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "70916701",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "699738a171b54e879d3ee9b2dffb0ec1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-247-39cbe821fa81>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_Xtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel_O\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_games\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplayer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'X'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-240-534a44d1d7e8>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(policy_net, target_net, trainer, memory, num_games, loss_reward, win_reward, move_reward, tie_reward, BATCH_SIZE, GAMMA, EPS_START, EPS_END, EPS_DECAY, TARGET_UPDATE, device, mem_len, player)\u001b[0m\n\u001b[0;32m    100\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpolicy_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m                     \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m                 \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\rmsprop.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m             F.rmsprop(params_with_grad,\n\u001b[0m\u001b[0;32m    136\u001b[0m                       \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m                       \u001b[0msquare_avgs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\_functional.py\u001b[0m in \u001b[0;36mrmsprop\u001b[1;34m(params, grads, square_avgs, grad_avgs, momentum_buffer_list, lr, alpha, eps, weight_decay, momentum, centered)\u001b[0m\n\u001b[0;32m    237\u001b[0m             \u001b[0mavg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msquare_avg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad_avg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_avg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 239\u001b[1;33m             \u001b[0mavg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msquare_avg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmomentum\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(model_X, model_Xtarget, trainer=model_O, num_games=5000, player='X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "33e43ff1",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68d133a26c004b3eb136dc4cdd295af1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(DQN(\n",
       "   (stack): Sequential(\n",
       "     (0): Conv1d(3, 20, kernel_size=(2,), stride=(1,))\n",
       "     (1): ReLU()\n",
       "     (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "     (3): Flatten(start_dim=1, end_dim=-1)\n",
       "     (4): Linear(in_features=20, out_features=32, bias=True)\n",
       "     (5): ReLU()\n",
       "     (6): Linear(in_features=32, out_features=64, bias=True)\n",
       "     (7): ReLU()\n",
       "     (8): Linear(in_features=64, out_features=32, bias=True)\n",
       "     (9): ReLU()\n",
       "     (10): Linear(in_features=32, out_features=9, bias=True)\n",
       "     (11): Softmax(dim=1)\n",
       "   )\n",
       " ),\n",
       " DQN(\n",
       "   (stack): Sequential(\n",
       "     (0): Conv1d(3, 20, kernel_size=(2,), stride=(1,))\n",
       "     (1): ReLU()\n",
       "     (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "     (3): Flatten(start_dim=1, end_dim=-1)\n",
       "     (4): Linear(in_features=20, out_features=32, bias=True)\n",
       "     (5): ReLU()\n",
       "     (6): Linear(in_features=32, out_features=64, bias=True)\n",
       "     (7): ReLU()\n",
       "     (8): Linear(in_features=64, out_features=32, bias=True)\n",
       "     (9): ReLU()\n",
       "     (10): Linear(in_features=32, out_features=9, bias=True)\n",
       "     (11): Softmax(dim=1)\n",
       "   )\n",
       " ),\n",
       " <__main__.ReplayMemory at 0x1ba33aa1a60>)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(model_O, model_Otarget, trainer=model_X, num_games=500, player='O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8afaa8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
